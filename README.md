# ğŸ“Š **Scopus Data Pipeline and Analytics Dashboard**

## ğŸš€ **AperÃ§u du Projet**

Ce projet met en place un pipeline de traitement de donnÃ©es complet en utilisant des outils modernes pour extraire, transformer et visualiser des donnÃ©es provenant de **Scopus** via une API. L'architecture ETL (Extract, Transform, Load) est construite pour assurer une transformation efficace des donnÃ©es et leur visualisation interactive.

---

## ğŸ“ **Architecture du Projet**

![Architecture](path_to_image.png) <!-- Remplacez path_to_image par l'URL de l'image sur GitHub -->

Le pipeline se compose des Ã©tapes suivantesÂ :

1. **Source de DonnÃ©es**  
   - **Scopus API** : Extraction des donnÃ©es brutes via des appels API.

2. **Stockage des DonnÃ©es Brutes**  
   - Utilisation de **Snowflake** pour stocker les donnÃ©es brutes extraites.

3. **Transformation des DonnÃ©es**  
   - **PySpark** est utilisÃ© pourÂ :
     - **Nettoyage des donnÃ©es** : Suppression des doublons et valeurs nulles.  
     - **PrÃ©traitement des donnÃ©es** : Tokenization, normalisation, filtrage.  
     - **ModÃ©lisation des donnÃ©es** : CrÃ©ation de structures de tables optimisÃ©es.  

4. **Stockage des Tables**  
   - Stockage des tables factuelles et dimensionnelles dans **Microsoft SQL Server**.

5. **Visualisation des DonnÃ©es**  
   - CrÃ©ation de tableaux de bord interactifs avec **Power BI** pour l'analyse des donnÃ©es.

---

## ğŸ› ï¸ **Outils et Technologies UtilisÃ©s**

| **Ã‰tape**                | **Technologie**                  |
|--------------------------|---------------------------------|
| Source de donnÃ©es        | Scopus                    |
| Stockage des donnÃ©es     | Snowflake                       |
| Transformation des donnÃ©es | PySpark                       |
| Stockage des tables      | Microsoft SQL Server            |
| Visualisation            | Power BI                        |


